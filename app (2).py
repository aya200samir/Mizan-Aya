import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import time
import warnings
warnings.filterwarnings('ignore')

# ==================== Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ====================
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªÙŠØ±Ø§Ø¯ SHAP Ù„Ù„ØªÙØ³ÙŠØ±
try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False

# ==================== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© ====================
st.set_page_config(
    page_title="Mizan AI - Ù†Ø¸Ø§Ù… Ø§Ù„Ø¹Ø¯Ø§Ù„Ø© Ø§Ù„Ø°ÙƒÙŠ",
    page_icon="âš–ï¸",
    layout="wide"
)

# ==================== CSS Ù…Ø®ØµØµ ====================
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Cairo:wght@300;400;600;700;900&display=swap');
    * { font-family: 'Cairo', sans-serif; }
    
    .header {
        background: linear-gradient(135deg, #1e3c72, #2a5298);
        color: white;
        padding: 2rem;
        border-radius: 0 0 30px 30px;
        text-align: center;
        margin-bottom: 2rem;
        box-shadow: 0 10px 30px rgba(0,0,0,0.2);
    }
    .header h1 { font-size: 3rem; font-weight: 900; margin-bottom: 0.5rem; }
    .header p { font-size: 1.2rem; opacity: 0.9; }
    
    .metric-card {
        background: white;
        border-radius: 15px;
        padding: 1.5rem;
        box-shadow: 0 5px 20px rgba(0,0,0,0.05);
        text-align: center;
        border: 1px solid #e0e0e0;
        height: 100%;
        transition: transform 0.3s ease;
    }
    .metric-card:hover {
        transform: translateY(-5px);
        box-shadow: 0 10px 30px rgba(0,0,0,0.1);
    }
    .metric-card .value { font-size: 2.2rem; font-weight: 900; color: #1e3c72; }
    .metric-card .label { color: #666; font-size: 1rem; }
    
    .bias-alert {
        background: #ffebee;
        border-right: 5px solid #f44336;
        padding: 1rem;
        border-radius: 10px;
        margin: 1rem 0;
        font-weight: 600;
        color: #b71c1c;
    }
    
    .fairness-badge {
        background: #e8f5e9;
        border-right: 5px solid #4caf50;
        padding: 1rem;
        border-radius: 10px;
        margin: 1rem 0;
        font-weight: 600;
        color: #1b5e20;
    }
    
    .explanation-box {
        background: #f5f7fa;
        padding: 1.5rem;
        border-radius: 15px;
        border: 1px solid #ddd;
        margin: 1rem 0;
        direction: rtl;
        text-align: right;
    }
    
    .what-if-card {
        background: linear-gradient(135deg, #f5f7fa 0%, #e9ecef 100%);
        padding: 1.5rem;
        border-radius: 15px;
        border: 1px solid #ced4da;
        margin: 1rem 0;
    }
    
    .footer {
        background: #1e3c72;
        color: white;
        padding: 1.5rem;
        border-radius: 30px 30px 0 0;
        margin-top: 3rem;
        text-align: center;
    }
    .stButton > button {
        background: linear-gradient(135deg, #1e3c72, #2a5298);
        color: white;
        font-weight: 600;
        width: 100%;
        border: none;
        border-radius: 8px;
        padding: 0.75rem;
        transition: all 0.3s ease;
    }
    .stButton > button:hover {
        background: linear-gradient(135deg, #2a5298, #1e3c72);
        box-shadow: 0 5px 15px rgba(30,60,114,0.4);
        transform: translateY(-2px);
    }
</style>
""", unsafe_allow_html=True)

# ==================== ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø·Ø¨Ù‚Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© ====================
def generate_official_data():
    """Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: Ø¨ÙŠØ§Ù†Ø§Øª Ø±Ø³Ù…ÙŠØ© Ù…Ø¬Ù…Ø¹Ø© (Ù…Ø­Ø§ÙƒØ§Ø© Ù„Ù†Ø´Ø±Ø© Ø§Ù„Ø¥Ø³ÙƒØ§Ù†)"""
    return pd.DataFrame({
        "Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©": ["Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©", "Ø§Ù„Ø¬ÙŠØ²Ø©", "Ø§Ù„Ø¥Ø³ÙƒÙ†Ø¯Ø±ÙŠØ©", "Ø£Ø³ÙŠÙˆØ·", "Ø³ÙˆÙ‡Ø§Ø¬", "Ù‚Ù†Ø§", "Ø£Ø³ÙˆØ§Ù†", "Ø§Ù„Ù…Ù†ÙŠØ§"],
        "Ø§Ù„ÙˆØ­Ø¯Ø§Øª": [15000, 12000, 10000, 8000, 7000, 6000, 5000, 5500],
        "Ù†Ø³Ø¨Ø©_Ø§Ù„Ù‚Ø¨ÙˆÙ„": [0.18, 0.22, 0.20, 0.25, 0.27, 0.28, 0.30, 0.26]
    })

@st.cache_data
def generate_synthetic_data(n_extra=8000):
    """
    ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª ØµÙ†Ø§Ø¹ÙŠØ© Ù…Ø­Ø§ÙƒØ§Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±Ø³Ù…ÙŠØ© + Ø®ØµØ§Ø¦Øµ Ø¥Ø¶Ø§ÙÙŠØ©
    """
    official = generate_official_data()
    official["Ø§Ù„Ù…ØªÙ‚Ø¯Ù…ÙˆÙ†_Ø§Ù„ØªÙ‚Ø¯ÙŠØ±ÙŠÙˆÙ†"] = (official["Ø§Ù„ÙˆØ­Ø¯Ø§Øª"] / official["Ù†Ø³Ø¨Ø©_Ø§Ù„Ù‚Ø¨ÙˆÙ„"]).astype(int)
    total = n_extra

    # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø©
    governorates_list = official["Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©"].tolist()
    
    # ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø§Øª Ø­Ø³Ø¨ Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ø±Ø³Ù…ÙŠ
    probs = official["Ø§Ù„Ù…ØªÙ‚Ø¯Ù…ÙˆÙ†_Ø§Ù„ØªÙ‚Ø¯ÙŠØ±ÙŠÙˆÙ†"] / official["Ø§Ù„Ù…ØªÙ‚Ø¯Ù…ÙˆÙ†_Ø§Ù„ØªÙ‚Ø¯ÙŠØ±ÙŠÙˆÙ†"].sum()
    governorates = np.random.choice(governorates_list, size=total, p=probs)

    # Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    income = np.random.normal(5500, 2000, total).clip(1500, 15000)
    family_size = np.random.randint(1, 7, total)
    employment = np.random.choice(["Ø±Ø³Ù…ÙŠ", "ØºÙŠØ± Ø±Ø³Ù…ÙŠ"], total, p=[0.6, 0.4])
    
    # Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©
    marital_status = np.random.choice(
        ["Ø£Ø¹Ø²Ø¨", "Ù…ØªØ²ÙˆØ¬", "Ù…Ø·Ù„Ù‚", "Ø£Ø±Ù…Ù„"],
        total,
        p=[0.25, 0.60, 0.08, 0.07]
    )
    
    # Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø©
    disability = np.random.choice([0, 1], total, p=[0.885, 0.115])
    disability_severity = np.zeros(total)
    for i in range(total):
        if disability[i] == 1:
            disability_severity[i] = np.random.choice([0.3, 0.5, 0.8, 0.6, 0.5, 0.7, 0.9],
                                                       p=[0.25,0.15,0.1,0.12,0.1,0.15,0.13])
    
    # Ù…Ù„ÙƒÙŠØ© Ø³Ø§Ø¨Ù‚Ø©
    previous_ownership = np.random.choice([0, 1], total, p=[0.93, 0.07])

    data = pd.DataFrame({
        "Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©": governorates,
        "Ø§Ù„Ø¯Ø®Ù„": income,
        "Ø­Ø¬Ù…_Ø§Ù„Ø£Ø³Ø±Ø©": family_size,
        "Ù†ÙˆØ¹_Ø§Ù„Ø¹Ù…Ù„": employment,
        "Ø§Ù„Ø­Ø§Ù„Ø©_Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©": marital_status,
        "Ø¥Ø¹Ø§Ù‚Ø©": disability,
        "Ø´Ø¯Ø©_Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø©": disability_severity,
        "Ù…Ù„ÙƒÙŠØ©_Ø³Ø§Ø¨Ù‚Ø©": previous_ownership
    })

    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ø³ØªØ­Ù‚Ø§Ù‚ Ø§Ù„ÙØ¹Ù„ÙŠ
    ages = np.random.randint(18, 70, total)
    data["Ø§Ù„Ø¹Ù…Ø±"] = ages
    data["Ø§Ù„Ø§Ø³ØªØ­Ù‚Ø§Ù‚_Ø§Ù„ÙØ¹Ù„ÙŠ"] = (
        (data["Ø§Ù„Ø¯Ø®Ù„"] <= 6000) & 
        (data["Ù…Ù„ÙƒÙŠØ©_Ø³Ø§Ø¨Ù‚Ø©"] == 0) &
        (data["Ø§Ù„Ø¹Ù…Ø±"] >= 21)
    ).astype(int)

    # Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª Ø¥Ù†Ø³Ø§Ù†ÙŠØ©
    special_cases = (data["Ø´Ø¯Ø©_Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø©"] > 0.7) & (data["Ø§Ù„Ø¯Ø®Ù„"] <= 7000)
    data.loc[special_cases, "Ø§Ù„Ø§Ø³ØªØ­Ù‚Ø§Ù‚_Ø§Ù„ÙØ¹Ù„ÙŠ"] = 1

    # ===== Ù†Ø¸Ø§Ù… Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„ØªØµØ§Ø¹Ø¯ÙŠ Ø§Ù„Ù…ØªØ±Ø§ÙƒÙ… (Cumulative Progressive Weights) =====
    # Ø§Ù„ÙÙ„Ø³ÙØ©: Ù†Ø¨Ø¯Ø£ Ù…Ù† Ø§Ù„Ø¹Ø¬Ø² Ø§Ù„Ø¨Ø¯Ù†ÙŠ (Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø´Ø¯ÙŠØ¯Ø©)ØŒ Ø«Ù… Ø§Ù„Ù‡Ø´Ø§Ø´Ø© Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ© (Ø§Ù„Ø£Ø±Ø§Ù…Ù„)ØŒ Ø«Ù… Ø§Ù„Ù…Ø¸Ù„ÙˆÙ…ÙŠØ© Ø§Ù„Ø¬ØºØ±Ø§ÙÙŠØ© (Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ù†Ø§Ø¦ÙŠØ©)
    
    data["ÙˆØ²Ù†_Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©"] = 1.0  # Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ

    # Ø§Ù„Ù…Ø±ØªØ¨Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø´Ø¯ÙŠØ¯Ø© (Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ø£Ø¹Ù„Ù‰ - 2.0x)
    # Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø´Ø¯ÙŠØ¯Ø© (Ø£ÙƒØ«Ø± Ù…Ù† 0.7) ØªØ­ØµÙ„ Ø¹Ù„Ù‰ ÙˆØ²Ù† Ù…Ø¶Ø§Ø¹Ù
    severe_disability_mask = data["Ø´Ø¯Ø©_Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø©"] >= 0.7
    data.loc[severe_disability_mask, "ÙˆØ²Ù†_Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©"] *= 2.0
    
    # Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ù…ØªÙˆØ³Ø·Ø© (Ø¨ÙŠÙ† 0.4 Ùˆ 0.7) ØªØ­ØµÙ„ Ø¹Ù„Ù‰ ÙˆØ²Ù† 1.5x
    moderate_disability_mask = (data["Ø´Ø¯Ø©_Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø©"] >= 0.4) & (data["Ø´Ø¯Ø©_Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø©"] < 0.7)
    data.loc[moderate_disability_mask, "ÙˆØ²Ù†_Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©"] *= 1.5
    
    # Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨Ø³ÙŠØ·Ø© (Ø£Ù‚Ù„ Ù…Ù† 0.4) ØªØ­ØµÙ„ Ø¹Ù„Ù‰ ÙˆØ²Ù† 1.2x
    mild_disability_mask = (data["Ø´Ø¯Ø©_Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø©"] > 0) & (data["Ø´Ø¯Ø©_Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø©"] < 0.4)
    data.loc[mild_disability_mask, "ÙˆØ²Ù†_Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©"] *= 1.2

    # Ø§Ù„Ù…Ø±ØªØ¨Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: Ø§Ù„Ø£Ø±Ù…Ù„Ø© Ø§Ù„ØªÙŠ ØªØ¹ÙˆÙ„ (ÙˆØ²Ù† - 1.8x)
    # Ø§Ù„Ø£Ø±Ø§Ù…Ù„ (Ø®Ø§ØµØ© Ù…Ø¹ ÙˆØ¬ÙˆØ¯ Ø£Ø·ÙØ§Ù„) ÙŠØ­ØµÙ„Ù† Ø¹Ù„Ù‰ ÙˆØ²Ù† ÙƒØ¨ÙŠØ±
    widowed_mask = data["Ø§Ù„Ø­Ø§Ù„Ø©_Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©"] == "Ø£Ø±Ù…Ù„"
    data.loc[widowed_mask, "ÙˆØ²Ù†_Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©"] *= 1.8
    
    # Ø§Ù„Ù…Ø·Ù„Ù‚Ø§Øª Ù…Ø¹ Ø£Ø·ÙØ§Ù„ ÙŠØ­ØµÙ„Ù† Ø¹Ù„Ù‰ ÙˆØ²Ù† 1.4x
    divorced_with_kids_mask = (data["Ø§Ù„Ø­Ø§Ù„Ø©_Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©"] == "Ù…Ø·Ù„Ù‚") & (data["Ø­Ø¬Ù…_Ø§Ù„Ø£Ø³Ø±Ø©"] > 2)
    data.loc[divorced_with_kids_mask, "ÙˆØ²Ù†_Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©"] *= 1.4

    # Ø§Ù„Ù…Ø±ØªØ¨Ø© Ø§Ù„Ø«Ø§Ù„Ø«Ø©: Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ù†Ø§Ø¦ÙŠØ© (ÙˆØ²Ù† - 1.5x)
    remote_areas = ["Ø£Ø³ÙŠÙˆØ·", "Ø³ÙˆÙ‡Ø§Ø¬", "Ù‚Ù†Ø§", "Ø£Ø³ÙˆØ§Ù†"]
    remote_mask = data["Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©"].isin(remote_areas)
    data.loc[remote_mask, "ÙˆØ²Ù†_Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©"] *= 1.5

    # Ø§Ù„Ø¹Ù…Ø§Ù„Ø© ØºÙŠØ± Ø§Ù„Ù…Ù†ØªØ¸Ù…Ø© (ÙˆØ²Ù† Ø¥Ø¶Ø§ÙÙŠ 1.2x)
    informal_mask = data["Ù†ÙˆØ¹_Ø§Ù„Ø¹Ù…Ù„"] == "ØºÙŠØ± Ø±Ø³Ù…ÙŠ"
    data.loc[informal_mask, "ÙˆØ²Ù†_Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©"] *= 1.2

    # ===== Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„ØªÙ‚Ø§Ø·Ø¹ÙŠØ© (Intersectionality) - ØªØ±Ø§ÙƒÙ… Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ù„Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø±ÙƒØ¨Ø© =====
    # Ù…Ø«Ø§Ù„: Ø£Ø±Ù…Ù„Ø© + Ø¥Ø¹Ø§Ù‚Ø© + Ù…Ù†Ø·Ù‚Ø© Ù†Ø§Ø¦ÙŠØ© = Ø£ÙˆØ²Ø§Ù† Ù…ØªØ±Ø§ÙƒÙ…Ø©
    # Ù‡Ø°Ø§ ÙŠØ­Ø¯Ø« ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ù„Ø£Ù†Ù†Ø§ Ù†Ø¶Ø±Ø¨ Ø§Ù„Ø£ÙˆØ²Ø§Ù† (Ø§Ù„Ø¶Ø±Ø¨ Ø§Ù„Ù…ØªØªØ§Ù„ÙŠ)
    # Ù†Ø­ØªØ§Ø¬ ÙÙ‚Ø· Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… ØªØ¬Ø§ÙˆØ² Ø­Ø¯ Ù…Ø¹ÙŠÙ† (Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªØ¶Ø®Ù… Ø§Ù„Ù…ÙØ±Ø·)
    data["ÙˆØ²Ù†_Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©"] = data["ÙˆØ²Ù†_Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©"].clip(upper=5.0)  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 5 Ø£Ø¶Ø¹Ø§Ù

    # Ø§Ù„Ù‚Ø±Ø§Ø± Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ
    data["Ø§Ù„Ù‚Ø±Ø§Ø±_Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ"] = (
        (data["Ø§Ù„Ø¯Ø®Ù„"] <= 6000) & 
        (data["Ù…Ù„ÙƒÙŠØ©_Ø³Ø§Ø¨Ù‚Ø©"] == 0)
    ).astype(int)

    return data

# ==================== Ø¯Ø§Ù„Ø© MCAS Ù„Ù…ÙƒØ§ÙØ­Ø© Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„ÙˆÙ‡Ù…ÙŠØ© ====================
def mcas_score(y_true, y_pred, lambda1=1, lambda2=1):
    """
    Ø­Ø³Ø§Ø¨ Ù…Ù‚ÙŠØ§Ø³ MCAS ÙˆÙÙ‚Ù‹Ø§ Ù„Ø¨Ø­Ø« Ø¯. Ù…Ø­Ù…Ø¯ Ø§Ù„Ù‡Ø§Ø¯Ø§Ø¯
    """
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    
    css_plus = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0
    css_minus = tn / (tn + fp + fn) if (tn + fp + fn) > 0 else 0
    
    cfs = 0.5 * (
        (fp / (tp + tn + fp) if (tp + tn + fp) > 0 else 0) +
        (fn / (tp + tn + fn) if (tp + tn + fn) > 0 else 0)
    )
    
    mcas = (lambda1 * (css_plus - cfs) + lambda2 * (css_minus - cfs)) / (lambda1 + lambda2)
    return mcas

# ==================== ØªØ­Ù„ÙŠÙ„ EDA ÙˆØ§ÙƒØªØ´Ø§Ù Ø§Ù„ØªØ­ÙŠØ² ====================
def analyze_bias(data):
    """ØªØ­Ù„ÙŠÙ„ Ù…Ø¹Ø¯Ù„Ø§Øª Ø§Ù„Ù‚Ø¨ÙˆÙ„ Ø­Ø³Ø¨ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©"""
    results = {}
    by_gov = data.groupby("Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©")["Ø§Ù„Ø§Ø³ØªØ­Ù‚Ø§Ù‚_Ø§Ù„ÙØ¹Ù„ÙŠ"].mean()
    results["Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©"] = by_gov
    by_work = data.groupby("Ù†ÙˆØ¹_Ø§Ù„Ø¹Ù…Ù„")["Ø§Ù„Ø§Ø³ØªØ­Ù‚Ø§Ù‚_Ø§Ù„ÙØ¹Ù„ÙŠ"].mean()
    results["Ù†ÙˆØ¹_Ø§Ù„Ø¹Ù…Ù„"] = by_work
    by_marital = data.groupby("Ø§Ù„Ø­Ø§Ù„Ø©_Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©")["Ø§Ù„Ø§Ø³ØªØ­Ù‚Ø§Ù‚_Ø§Ù„ÙØ¹Ù„ÙŠ"].mean()
    results["Ø§Ù„Ø­Ø§Ù„Ø©_Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©"] = by_marital
    by_disability = data.groupby("Ø¥Ø¹Ø§Ù‚Ø©")["Ø§Ù„Ø§Ø³ØªØ­Ù‚Ø§Ù‚_Ø§Ù„ÙØ¹Ù„ÙŠ"].mean()
    results["Ø¥Ø¹Ø§Ù‚Ø©"] = by_disability
    return results

def detect_bias_gap(data, feature):
    """Ø­Ø³Ø§Ø¨ Ø§Ù„ÙØ¬ÙˆØ© Ø¨ÙŠÙ† Ø£Ø¹Ù„Ù‰ ÙˆØ£Ù‚Ù„ Ù†Ø³Ø¨Ø© Ù‚Ø¨ÙˆÙ„ Ù„Ù…ÙŠØ²Ø© Ù…Ø¹ÙŠÙ†Ø©"""
    rates = data.groupby(feature)["Ø§Ù„Ø§Ø³ØªØ­Ù‚Ø§Ù‚_Ø§Ù„ÙØ¹Ù„ÙŠ"].mean()
    return rates.max() - rates.min()

# ==================== ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¹Ø§Ø¯Ù„ ====================
@st.cache_resource
def train_fair_model(data):
    """
    ØªØ¯Ø±ÙŠØ¨ RandomForest Ù…Ø¹ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø¹ÙŠÙ†Ø§Øª (sample_weight) Ø§Ù„Ù…Ø³ØªÙ…Ø¯Ø© Ù…Ù† "ÙˆØ²Ù†_Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©"
    """
    feature_cols = ['Ø§Ù„Ø¹Ù…Ø±', 'Ø§Ù„Ø¯Ø®Ù„', 'Ø­Ø¬Ù…_Ø§Ù„Ø£Ø³Ø±Ø©', 'Ø¥Ø¹Ø§Ù‚Ø©', 'Ø´Ø¯Ø©_Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø©', 'Ù…Ù„ÙƒÙŠØ©_Ø³Ø§Ø¨Ù‚Ø©']
    
    data_encoded = data.copy()
    encoders = {}
    for col in ['Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©', 'Ù†ÙˆØ¹_Ø§Ù„Ø¹Ù…Ù„', 'Ø§Ù„Ø­Ø§Ù„Ø©_Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©']:
        le = LabelEncoder()
        data_encoded[col] = le.fit_transform(data_encoded[col])
        encoders[col] = le
        feature_cols.append(col)
    
    X = data_encoded[feature_cols]
    y = data_encoded['Ø§Ù„Ø§Ø³ØªØ­Ù‚Ø§Ù‚_Ø§Ù„ÙØ¹Ù„ÙŠ']
    sample_weights = data_encoded['ÙˆØ²Ù†_Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©'].values

    X_train, X_test, y_train, y_test, w_train, w_test = train_test_split(
        X, y, sample_weights, test_size=0.2, random_state=42, stratify=y
    )

    fair_model = RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42, n_jobs=-1)
    fair_model.fit(X_train, y_train, sample_weight=w_train)

    y_pred_fair = fair_model.predict(X_test)
    y_proba_fair = fair_model.predict_proba(X_test)[:, 1]

    metrics_fair = {
        'accuracy': accuracy_score(y_test, y_pred_fair),
        'precision': precision_score(y_test, y_pred_fair),
        'recall': recall_score(y_test, y_pred_fair),
        'f1': f1_score(y_test, y_pred_fair),
        'mcas': mcas_score(y_test, y_pred_fair)
    }

    return {
        'model': fair_model,
        'encoders': encoders,
        'feature_cols': feature_cols,
        'metrics': metrics_fair,
        'X_test': X_test,
        'y_test': y_test,
        'y_pred': y_pred_fair,
        'y_proba': y_proba_fair,
        'sample_weights': w_test,
        'X_train': X_train,
        'y_train': y_train
    }

# ==================== ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ ====================
def train_traditional_model(data):
    """Ù†Ù…ÙˆØ°Ø¬ ØªÙ‚Ù„ÙŠØ¯ÙŠ Ø¨Ø³ÙŠØ·: Ù„Ø§ ÙŠØ³ØªØ®Ø¯Ù… Ø£ÙˆØ²Ø§Ù† Ø¹Ø¯Ø§Ù„Ø©"""
    feature_cols = ['Ø§Ù„Ø¹Ù…Ø±', 'Ø§Ù„Ø¯Ø®Ù„', 'Ø­Ø¬Ù…_Ø§Ù„Ø£Ø³Ø±Ø©', 'Ø¥Ø¹Ø§Ù‚Ø©', 'Ø´Ø¯Ø©_Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø©', 'Ù…Ù„ÙƒÙŠØ©_Ø³Ø§Ø¨Ù‚Ø©']
    data_encoded = data.copy()
    encoders = {}
    for col in ['Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©', 'Ù†ÙˆØ¹_Ø§Ù„Ø¹Ù…Ù„', 'Ø§Ù„Ø­Ø§Ù„Ø©_Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©']:
        le = LabelEncoder()
        data_encoded[col] = le.fit_transform(data_encoded[col])
        encoders[col] = le
        feature_cols.append(col)
    
    X = data_encoded[feature_cols]
    y = data_encoded['Ø§Ù„Ø§Ø³ØªØ­Ù‚Ø§Ù‚_Ø§Ù„ÙØ¹Ù„ÙŠ']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    
    trad_model = RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42, n_jobs=-1)
    trad_model.fit(X_train, y_train)
    y_pred = trad_model.predict(X_test)
    y_proba = trad_model.predict_proba(X_test)[:, 1]
    
    metrics = {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred),
        'recall': recall_score(y_test, y_pred),
        'f1': f1_score(y_test, y_pred),
        'mcas': mcas_score(y_test, y_pred)
    }
    
    return {
        'model': trad_model,
        'encoders': encoders,
        'feature_cols': feature_cols,
        'metrics': metrics,
        'X_test': X_test,
        'y_test': y_test,
        'y_pred': y_pred,
        'y_proba': y_proba,
        'X_train': X_train,
        'y_train': y_train
    }

# ==================== Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙ†Ø¨Ø¤ + Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ† ====================
def hybrid_decision(model_pack, user_data, threshold_high=0.8, threshold_low=0.2):
    """
    - Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø«Ù‚Ø© â‰¥ 0.8 â†’ Ù…Ù‚Ø¨ÙˆÙ„ Ø¢Ù„ÙŠØ§Ù‹ Ù…Ø¹ ØªÙØ³ÙŠØ±
    - Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø«Ù‚Ø© â‰¤ 0.2 â†’ Ù…Ø±ÙÙˆØ¶ Ø¢Ù„ÙŠØ§Ù‹ Ù…Ø¹ ØªÙØ³ÙŠØ±
    - ÙˆØ¥Ù„Ø§ â†’ ÙŠØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø¨Ø´Ø±ÙŠØ© Ù…Ø¹ ØªÙ‚Ø±ÙŠØ±
    """
    model = model_pack['model']
    encoders = model_pack['encoders']
    feature_cols = model_pack['feature_cols']
    
    input_df = pd.DataFrame([user_data])
    for col, encoder in encoders.items():
        if col in input_df.columns:
            input_df[col] = encoder.transform(input_df[col])
    
    X_input = input_df[feature_cols]
    prob = model.predict_proba(X_input)[0][1]
    pred = model.predict(X_input)[0]
    
    if prob >= threshold_high:
        decision = "âœ… Ù…Ù‚Ø¨ÙˆÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹"
        confidence = prob
        review_needed = False
    elif prob <= threshold_low:
        decision = "âŒ Ù…Ø±ÙÙˆØ¶ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹"
        confidence = 1 - prob
        review_needed = False
    else:
        decision = "âš ï¸ ÙŠØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø¨Ø´Ø±ÙŠØ©"
        confidence = prob
        review_needed = True
    
    return {
        'prediction': pred,
        'probability': prob,
        'decision': decision,
        'confidence': confidence,
        'review_needed': review_needed
    }

def generate_explanation(user_data, hybrid_result):
    """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± ØªÙØ³ÙŠØ±ÙŠ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
    factors = []
    
    # Ø§Ù„Ø´Ø±ÙˆØ· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    if user_data['Ø§Ù„Ø¯Ø®Ù„'] <= 6000:
        factors.append("âœ“ Ø§Ù„Ø¯Ø®Ù„ Ù…Ù†Ø§Ø³Ø¨ (â‰¤ 6000)")
    else:
        factors.append("âœ— Ø§Ù„Ø¯Ø®Ù„ Ù…Ø±ØªÙØ¹")
    
    if user_data['Ù…Ù„ÙƒÙŠØ©_Ø³Ø§Ø¨Ù‚Ø©'] == 0:
        factors.append("âœ“ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„ÙƒÙŠØ© Ø³Ø§Ø¨Ù‚Ø©")
    else:
        factors.append("âœ— Ù„Ø¯ÙŠÙ‡ Ù…Ù„ÙƒÙŠØ© Ø³Ø§Ø¨Ù‚Ø©")
    
    if user_data['Ø§Ù„Ø¹Ù…Ø±'] >= 21:
        factors.append("âœ“ Ø§Ù„Ø¹Ù…Ø± Ù…Ù†Ø§Ø³Ø¨")
    else:
        factors.append("âœ— Ø§Ù„Ø¹Ù…Ø± Ø£Ù‚Ù„ Ù…Ù† 21")
    
    # Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„ØªØµØ§Ø¹Ø¯ÙŠØ©
    if user_data['Ø¥Ø¹Ø§Ù‚Ø©'] == 1:
        severity = user_data['Ø´Ø¯Ø©_Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø©']
        if severity >= 0.7:
            factors.append(f"âœ“âœ“ Ø¥Ø¹Ø§Ù‚Ø© Ø´Ø¯ÙŠØ¯Ø© (Ø¯Ø±Ø¬Ø© {severity:.1f}) - Ø£ÙˆÙ„ÙˆÙŠØ© Ù‚ØµÙˆÙ‰")
        elif severity >= 0.4:
            factors.append(f"âœ“ Ø¥Ø¹Ø§Ù‚Ø© Ù…ØªÙˆØ³Ø·Ø© (Ø¯Ø±Ø¬Ø© {severity:.1f}) - Ø£ÙˆÙ„ÙˆÙŠØ© Ø¹Ø§Ù„ÙŠØ©")
        else:
            factors.append(f"âœ“ Ø¥Ø¹Ø§Ù‚Ø© Ø¨Ø³ÙŠØ·Ø© (Ø¯Ø±Ø¬Ø© {severity:.1f}) - Ø£ÙˆÙ„ÙˆÙŠØ©")
    
    if user_data['Ø§Ù„Ø­Ø§Ù„Ø©_Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©'] == 'Ø£Ø±Ù…Ù„':
        factors.append("âœ“âœ“ Ø£Ø±Ù…Ù„/Ø£Ø±Ù…Ù„Ø© - Ø£ÙˆÙ„ÙˆÙŠØ© Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ© Ù‚ØµÙˆÙ‰")
    elif user_data['Ø§Ù„Ø­Ø§Ù„Ø©_Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©'] == 'Ù…Ø·Ù„Ù‚' and user_data.get('Ø­Ø¬Ù…_Ø§Ù„Ø£Ø³Ø±Ø©', 1) > 2:
        factors.append("âœ“ Ù…Ø·Ù„Ù‚/Ù…Ø·Ù„Ù‚Ø© Ù…Ø¹ Ø£Ø·ÙØ§Ù„ - Ø£ÙˆÙ„ÙˆÙŠØ© Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©")
    
    remote_areas = ["Ø£Ø³ÙŠÙˆØ·", "Ø³ÙˆÙ‡Ø§Ø¬", "Ù‚Ù†Ø§", "Ø£Ø³ÙˆØ§Ù†"]
    if user_data['Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©'] in remote_areas:
        factors.append("âœ“ Ù…Ù† Ù…Ù†Ø·Ù‚Ø© Ù†Ø§Ø¦ÙŠØ© - Ø£ÙˆÙ„ÙˆÙŠØ© Ø¬ØºØ±Ø§ÙÙŠØ©")
    
    if user_data['Ù†ÙˆØ¹_Ø§Ù„Ø¹Ù…Ù„'] == 'ØºÙŠØ± Ø±Ø³Ù…ÙŠ':
        factors.append("âœ“ Ø¹Ù…Ø§Ù„Ø© ØºÙŠØ± Ù…Ù†ØªØ¸Ù…Ø© - Ø£ÙˆÙ„ÙˆÙŠØ© Ø§Ù‚ØªØµØ§Ø¯ÙŠØ©")
    
    explanation = f"""
    ### ğŸ“‹ ØªÙ‚Ø±ÙŠØ± ØªÙØ³ÙŠØ± Ø§Ù„Ù‚Ø±Ø§Ø±
    **Ø§Ù„Ù†ØªÙŠØ¬Ø©:** {hybrid_result['decision']}  
    **Ø§Ù„Ø«Ù‚Ø©:** {hybrid_result['confidence']*100:.1f}%  
    
    **Ø§Ù„Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ù…Ø¤Ø«Ø±Ø©:**
    """ + "\n".join([f"- {f}" for f in factors])
    
    if hybrid_result['review_needed']:
        explanation += "\n\n**ğŸ”” ØªÙ… ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø·Ù„Ø¨ Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø¨Ø´Ø±ÙŠØ© Ù„Ø¹Ø¯Ù… ÙˆØ¶ÙˆØ­ Ø§Ù„Ø­Ø§Ù„Ø©.**"
    else:
        explanation += "\n\n**ğŸ¤– ØªÙ… Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø± Ø¢Ù„ÙŠØ§Ù‹ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ÙˆØ¶ÙˆØ­ Ø§Ù„Ø­Ø§Ù„Ø©.**"
    
    return explanation

# ==================== ØªØ­Ù„ÙŠÙ„ "Ù…Ø§Ø°Ø§ Ù„Ùˆ" (What-if Analysis) ====================
def what_if_analysis(model_pack, base_user_data):
    """
    ØªØ­Ù„ÙŠÙ„ ØªØ£Ø«ÙŠØ± ØªØºÙŠÙŠØ± Ø§Ù„Ø¯Ø®Ù„ Ø¹Ù„Ù‰ ÙØ±Øµ Ø§Ù„Ø§Ø³ØªØ­Ù‚Ø§Ù‚
    """
    model = model_pack['model']
    encoders = model_pack['encoders']
    feature_cols = model_pack['feature_cols']
    
    income_range = np.arange(2000, 10001, 500)
    probabilities = []
    
    for inc in income_range:
        temp_data = base_user_data.copy()
        temp_data['Ø§Ù„Ø¯Ø®Ù„'] = inc
        
        input_df = pd.DataFrame([temp_data])
        for col, encoder in encoders.items():
            if col in input_df.columns:
                input_df[col] = encoder.transform(input_df[col])
        
        X_input = input_df[feature_cols]
        prob = model.predict_proba(X_input)[0][1]
        probabilities.append(prob)
    
    return income_range, probabilities

# ==================== Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªØ·Ø¨ÙŠÙ‚ ====================
def main():
    st.markdown("""
    <div class="header">
        <h1>âš–ï¸ Mizan AI - Ù†Ø¸Ø§Ù… Ø§Ù„Ø¹Ø¯Ø§Ù„Ø© Ø§Ù„Ø°ÙƒÙŠ Ù„Ù„Ø¥Ø³ÙƒØ§Ù† Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ</h1>
        <p>Ù†Ù…ÙˆØ°Ø¬ Ù‡Ø¬ÙŠÙ† Ù…Ø¹ Ù†Ø¸Ø§Ù… Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„ØªØµØ§Ø¹Ø¯ÙŠ Ø§Ù„Ù…ØªØ±Ø§ÙƒÙ… (Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© â† Ø§Ù„Ø£Ø±Ø§Ù…Ù„ â† Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ù†Ø§Ø¦ÙŠØ©)</p>
    </div>
    """, unsafe_allow_html=True)

    # ===== ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª =====
    with st.spinner("ğŸ“Š Ø¬Ø§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø©..."):
        data = generate_synthetic_data(n_extra=8000)
        official = generate_official_data()

    # ===== ØªØ­Ù„ÙŠÙ„ EDA ÙˆØ§Ù„ØªØ­ÙŠØ² =====
    st.markdown("## ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§ÙÙŠ (EDA) ÙˆØ§ÙƒØªØ´Ø§Ù Ø§Ù„ØªØ­ÙŠØ²")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª", f"{len(data):,}")
    with col2:
        st.metric("Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø³ØªØ­Ù‚ÙŠÙ† Ø§Ù„ÙØ¹Ù„ÙŠØ©", f"{data['Ø§Ù„Ø§Ø³ØªØ­Ù‚Ø§Ù‚_Ø§Ù„ÙØ¹Ù„ÙŠ'].mean()*100:.1f}%")
    with col3:
        st.metric("Ù†Ø³Ø¨Ø© Ø§Ù„Ù‚Ø±Ø§Ø± Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ", f"{data['Ø§Ù„Ù‚Ø±Ø§Ø±_Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ'].mean()*100:.1f}%")
    
    bias_gap = detect_bias_gap(data, "Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©")
    st.metric("Ø§Ù„ÙØ¬ÙˆØ© Ø¨ÙŠÙ† Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø§Øª (ØªØ­ÙŠØ²)", f"{bias_gap*100:.1f}%")
    
    st.markdown("### ğŸ“ˆ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ­Ù‚Ø§Ù‚ Ø­Ø³Ø¨ Ø§Ù„ÙØ¦Ø§Øª")
    tab1, tab2, tab3, tab4 = st.tabs(["Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©", "Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù…Ù„", "Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©", "Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø©"])
    
    bias_results = analyze_bias(data)
    with tab1:
        fig = px.bar(x=bias_results["Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©"].index, y=bias_results["Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©"].values,
                     title="Ù†Ø³Ø¨Ø© Ø§Ù„Ø§Ø³ØªØ­Ù‚Ø§Ù‚ Ø­Ø³Ø¨ Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©", color=bias_results["Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©"].values,
                     color_continuous_scale="RdYlGn")
        st.plotly_chart(fig, use_container_width=True)
        if bias_gap > 0.1:
            st.markdown(f'<div class="bias-alert">âš ï¸ ØªØ­Ø°ÙŠØ±: ÙØ¬ÙˆØ© ÙƒØ¨ÙŠØ±Ø© Ø¨ÙŠÙ† Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø§Øª ({bias_gap*100:.1f}%)</div>', unsafe_allow_html=True)
    
    with tab2:
        fig = px.bar(x=bias_results["Ù†ÙˆØ¹_Ø§Ù„Ø¹Ù…Ù„"].index, y=bias_results["Ù†ÙˆØ¹_Ø§Ù„Ø¹Ù…Ù„"].values,
                     title="Ù†Ø³Ø¨Ø© Ø§Ù„Ø§Ø³ØªØ­Ù‚Ø§Ù‚ Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù…Ù„", color=bias_results["Ù†ÙˆØ¹_Ø§Ù„Ø¹Ù…Ù„"].values,
                     color_continuous_scale="RdYlGn")
        st.plotly_chart(fig, use_container_width=True)
    with tab3:
        fig = px.bar(x=bias_results["Ø§Ù„Ø­Ø§Ù„Ø©_Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©"].index, y=bias_results["Ø§Ù„Ø­Ø§Ù„Ø©_Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©"].values,
                     title="Ù†Ø³Ø¨Ø© Ø§Ù„Ø§Ø³ØªØ­Ù‚Ø§Ù‚ Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©", color=bias_results["Ø§Ù„Ø­Ø§Ù„Ø©_Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©"].values,
                     color_continuous_scale="RdYlGn")
        st.plotly_chart(fig, use_container_width=True)
    with tab4:
        fig = px.bar(x=['ØºÙŠØ± Ù…Ø¹Ø§Ù‚', 'Ù…Ø¹Ø§Ù‚'], y=bias_results["Ø¥Ø¹Ø§Ù‚Ø©"].values,
                     title="Ù†Ø³Ø¨Ø© Ø§Ù„Ø§Ø³ØªØ­Ù‚Ø§Ù‚ Ø­Ø³Ø¨ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø©", color=bias_results["Ø¥Ø¹Ø§Ù‚Ø©"].values,
                     color_continuous_scale="RdYlGn")
        st.plotly_chart(fig, use_container_width=True)

    # Ø¹Ø±Ø¶ ØªÙˆØ²ÙŠØ¹ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©
    st.markdown("### âš–ï¸ ØªÙˆØ²ÙŠØ¹ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©")
    fig = px.histogram(data, x="ÙˆØ²Ù†_Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©", nbins=50, title="ØªÙˆØ²ÙŠØ¹ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©",
                       color_discrete_sequence=["#4caf50"])
    fig.add_vline(x=1.0, line_dash="dash", line_color="red", annotation_text="Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ")
    st.plotly_chart(fig, use_container_width=True)

    # ===== ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ (ØªÙ‚Ù„ÙŠØ¯ÙŠ ÙˆØ¹Ø§Ø¯Ù„) =====
    st.markdown("---")
    st.markdown("## ğŸ¤– ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©")

    with st.spinner("ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ..."):
        trad_pack = train_traditional_model(data)
    with st.spinner("âš–ï¸ Ø¬Ø§Ø±ÙŠ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¹Ø§Ø¯Ù„ (Ù…Ø¹ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø¹Ø¯Ø§Ù„Ø© Ø§Ù„ØªØµØ§Ø¹Ø¯ÙŠØ©)..."):
        fair_pack = train_fair_model(data)

    # Ø¹Ø±Ø¶ Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡
    st.markdown("### ğŸ“Š Ù…Ù‚Ø§Ø±Ù†Ø© Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠÙ†")
    comp_df = pd.DataFrame({
        'Ø§Ù„Ù…Ù‚ÙŠØ§Ø³': ['Ø§Ù„Ø¯Ù‚Ø© (Accuracy)', 'Ø§Ù„Ø¯Ù‚Ø© (Precision)', 'Ø§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ (Recall)', 'F1', 'MCAS'],
        'Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ': [
            f"{trad_pack['metrics']['accuracy']*100:.2f}%",
            f"{trad_pack['metrics']['precision']*100:.2f}%",
            f"{trad_pack['metrics']['recall']*100:.2f}%",
            f"{trad_pack['metrics']['f1']*100:.2f}%",
            f"{trad_pack['metrics']['mcas']*100:.2f}%"
        ],
        'Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¹Ø§Ø¯Ù„': [
            f"{fair_pack['metrics']['accuracy']*100:.2f}%",
            f"{fair_pack['metrics']['precision']*100:.2f}%",
            f"{fair_pack['metrics']['recall']*100:.2f}%",
            f"{fair_pack['metrics']['f1']*100:.2f}%",
            f"{fair_pack['metrics']['mcas']*100:.2f}%"
        ]
    })
    st.dataframe(comp_df, use_container_width=True)

    # Ù…ØµÙÙˆÙØ§Øª Ø§Ù„Ø§Ø±ØªØ¨Ø§Ùƒ
    col1, col2 = st.columns(2)
    with col1:
        st.markdown("**Ù…ØµÙÙˆÙØ© Ø§Ø±ØªØ¨Ø§Ùƒ - Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ**")
        cm_trad = confusion_matrix(trad_pack['y_test'], trad_pack['y_pred'])
        fig = px.imshow(cm_trad, text_auto=True, x=['ØºÙŠØ± Ù…Ø³ØªØ­Ù‚', 'Ù…Ø³ØªØ­Ù‚'], y=['ØºÙŠØ± Ù…Ø³ØªØ­Ù‚', 'Ù…Ø³ØªØ­Ù‚'],
                        color_continuous_scale='Blues')
        st.plotly_chart(fig, use_container_width=True)
    with col2:
        st.markdown("**Ù…ØµÙÙˆÙØ© Ø§Ø±ØªØ¨Ø§Ùƒ - Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¹Ø§Ø¯Ù„**")
        cm_fair = confusion_matrix(fair_pack['y_test'], fair_pack['y_pred'])
        fig = px.imshow(cm_fair, text_auto=True, x=['ØºÙŠØ± Ù…Ø³ØªØ­Ù‚', 'Ù…Ø³ØªØ­Ù‚'], y=['ØºÙŠØ± Ù…Ø³ØªØ­Ù‚', 'Ù…Ø³ØªØ­Ù‚'],
                        color_continuous_scale='Greens')
        st.plotly_chart(fig, use_container_width=True)

    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ­ÙŠØ² Ø¨Ø¹Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    st.markdown("### âš–ï¸ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ø¯Ø§Ù„Ø© Ø¨Ø¹Ø¯ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¹Ø§Ø¯Ù„")
    test_data = data.iloc[fair_pack['X_test'].index].copy()
    test_data['ØªÙ†Ø¨Ø¤_Ø¹Ø§Ø¯Ù„'] = fair_pack['y_pred']
    
    acc_by_gov_fair = test_data.groupby('Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©')['ØªÙ†Ø¨Ø¤_Ø¹Ø§Ø¯Ù„'].mean()
    acc_by_gov_true = test_data.groupby('Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©')['Ø§Ù„Ø§Ø³ØªØ­Ù‚Ø§Ù‚_Ø§Ù„ÙØ¹Ù„ÙŠ'].mean()
    
    fig = go.Figure()
    fig.add_trace(go.Bar(x=acc_by_gov_fair.index, y=acc_by_gov_fair.values, name='Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¹Ø§Ø¯Ù„', marker_color='#4caf50'))
    fig.add_trace(go.Bar(x=acc_by_gov_true.index, y=acc_by_gov_true.values, name='Ø§Ù„Ø§Ø³ØªØ­Ù‚Ø§Ù‚ Ø§Ù„ÙØ¹Ù„ÙŠ', marker_color='#2196f3'))
    fig.update_layout(title='Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù‚Ø¨ÙˆÙ„ Ø­Ø³Ø¨ Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©: Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¹Ø§Ø¯Ù„ vs Ø§Ù„Ø§Ø³ØªØ­Ù‚Ø§Ù‚ Ø§Ù„ÙØ¹Ù„ÙŠ',
                      xaxis_title='Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©', yaxis_title='Ù†Ø³Ø¨Ø© Ø§Ù„Ù‚Ø¨ÙˆÙ„', barmode='group')
    st.plotly_chart(fig, use_container_width=True)

    new_gap = acc_by_gov_fair.max() - acc_by_gov_fair.min()
    st.metric("Ø§Ù„ÙØ¬ÙˆØ© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø¨ÙŠÙ† Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø§Øª (Ø¨Ø¹Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¹Ø§Ø¯Ù„)", f"{new_gap*100:.1f}%",
              delta=f"{(bias_gap - new_gap)*100:.1f}% Ø§Ù†Ø®ÙØ§Ø¶", delta_color="normal")

    # ===== Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ† =====
    st.markdown("---")
    st.markdown("## ğŸ§  Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ† Ù„Ù„Ù‚Ø±Ø§Ø±Ø§Øª")
    st.info("""
    **Ø¢Ù„ÙŠØ© Ø§Ù„Ø¹Ù…Ù„:**
    - **Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø®Ø¶Ø±Ø§Ø¡ (Ø«Ù‚Ø© â‰¥ 80%)** â†’ Ù‚Ø±Ø§Ø± Ø¢Ù„ÙŠ (Ù…Ù‚Ø¨ÙˆÙ„) Ù…Ø¹ ØªÙØ³ÙŠØ±.
    - **Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø­Ù…Ø±Ø§Ø¡ (Ø«Ù‚Ø© â‰¤ 20%)** â†’ Ù‚Ø±Ø§Ø± Ø¢Ù„ÙŠ (Ù…Ø±ÙÙˆØ¶) Ù…Ø¹ ØªÙØ³ÙŠØ±.
    - **Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø±Ù…Ø§Ø¯ÙŠØ© (Ø¨ÙŠÙ† 20% Ùˆ 80%)** â†’ ØªØ­ÙˆÙŠÙ„ Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø¨Ø´Ø±ÙŠØ© Ù…Ø¹ ØªÙ‚Ø±ÙŠØ± ØªÙØ³ÙŠØ±ÙŠ Ù…ÙØµÙ„.
    """)

    # Ø¥Ø¯Ø®Ø§Ù„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
    with st.expander("â• Ø£Ø¯Ø®Ù„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…", expanded=True):
        col1, col2 = st.columns(2)
        with col1:
            age = st.number_input("Ø§Ù„Ø¹Ù…Ø±", 18, 70, 35)
            governorate = st.selectbox("Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©", ['Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©', 'Ø§Ù„Ø¬ÙŠØ²Ø©', 'Ø§Ù„Ø¥Ø³ÙƒÙ†Ø¯Ø±ÙŠØ©', 'Ø£Ø³ÙŠÙˆØ·', 'Ø³ÙˆÙ‡Ø§Ø¬', 'Ù‚Ù†Ø§', 'Ø£Ø³ÙˆØ§Ù†', 'Ø§Ù„Ù…Ù†ÙŠØ§'])
            employment = st.selectbox("Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù…Ù„", ['Ø±Ø³Ù…ÙŠ', 'ØºÙŠØ± Ø±Ø³Ù…ÙŠ'])
            marital = st.selectbox("Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©", ['Ø£Ø¹Ø²Ø¨', 'Ù…ØªØ²ÙˆØ¬', 'Ù…Ø·Ù„Ù‚', 'Ø£Ø±Ù…Ù„'])
        with col2:
            income = st.number_input("Ø§Ù„Ø¯Ø®Ù„ Ø§Ù„Ø´Ù‡Ø±ÙŠ", 1500, 15000, 5000)
            family_size = st.number_input("Ø­Ø¬Ù… Ø§Ù„Ø£Ø³Ø±Ø©", 1, 6, 3)
            disability = st.checkbox("Ù„Ø¯ÙŠÙ‡ Ø¥Ø¹Ø§Ù‚Ø©")
            disability_severity = st.slider("Ø´Ø¯Ø© Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© (Ø¥Ø°Ø§ ÙˆØ¬Ø¯Øª)", 0.0, 1.0, 0.5, step=0.1,
                                            disabled=not disability)
            previous = st.checkbox("Ù…Ù„ÙƒÙŠØ© Ø³Ø§Ø¨Ù‚Ø©")

    col1, col2 = st.columns(2)
    with col1:
        predict_button = st.button("ğŸ”® ØªÙ†Ø¨Ø¤ ÙˆØªØ­Ù„ÙŠÙ„", use_container_width=True)
    with col2:
        what_if_button = st.button("ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ù…Ø§Ø°Ø§ Ù„Ùˆ (What-if)", use_container_width=True)

    if predict_button or what_if_button:
        # ØªØ¬Ù‡ÙŠØ² Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        user_data = {
            'Ø§Ù„Ø¹Ù…Ø±': age,
            'Ø§Ù„Ø¯Ø®Ù„': income,
            'Ø­Ø¬Ù…_Ø§Ù„Ø£Ø³Ø±Ø©': family_size,
            'Ø¥Ø¹Ø§Ù‚Ø©': 1 if disability else 0,
            'Ø´Ø¯Ø©_Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø©': disability_severity if disability else 0,
            'Ù…Ù„ÙƒÙŠØ©_Ø³Ø§Ø¨Ù‚Ø©': 1 if previous else 0,
            'Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©': governorate,
            'Ù†ÙˆØ¹_Ø§Ù„Ø¹Ù…Ù„': employment,
            'Ø§Ù„Ø­Ø§Ù„Ø©_Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©': marital
        }

        if predict_button:
            # Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¹Ø§Ø¯Ù„
            result = hybrid_decision(fair_pack, user_data)
            explanation = generate_explanation(user_data, result)

            # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªÙŠØ¬Ø©
            if "Ù…Ù‚Ø¨ÙˆÙ„" in result['decision']:
                st.success(f"### {result['decision']}")
            elif "Ù…Ø±ÙÙˆØ¶" in result['decision']:
                st.error(f"### {result['decision']}")
            else:
                st.warning(f"### {result['decision']}")

            st.progress(result['probability'])
            st.markdown(f"**Ø§Ù„Ø«Ù‚Ø©:** {result['confidence']*100:.1f}%")
            st.markdown(explanation, unsafe_allow_html=True)

            if result['review_needed']:
                st.markdown("""
                <div style="background:#fff3cd; padding:1rem; border-radius:10px; border-right:5px solid #ff9800;">
                    <strong>ğŸ“¢ ØªÙˆØµÙŠØ©:</strong> ÙŠÙØ±Ø¬Ù‰ Ø¹Ø±Ø¶ Ø§Ù„Ø·Ù„Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù„Ø¬Ù†Ø© Ø§Ù„Ù…Ø®ØªØµØ© Ù…Ø¹ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø£Ø¹Ù„Ø§Ù‡.
                </div>
                """, unsafe_allow_html=True)

        if what_if_button:
            st.markdown("### ğŸ“ˆ ØªØ­Ù„ÙŠÙ„ Ù…Ø§Ø°Ø§ Ù„Ùˆ (What-if)")
            income_range, probs = what_if_analysis(fair_pack, user_data)
            
            fig = px.line(x=income_range, y=probs, markers=True,
                         title="ØªØ£Ø«ÙŠØ± ØªØºÙŠÙŠØ± Ø§Ù„Ø¯Ø®Ù„ Ø¹Ù„Ù‰ ÙØ±Øµ Ø§Ù„Ø§Ø³ØªØ­Ù‚Ø§Ù‚",
                         labels={'x': 'Ø§Ù„Ø¯Ø®Ù„ Ø§Ù„Ø´Ù‡Ø±ÙŠ', 'y': 'Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ø§Ø³ØªØ­Ù‚Ø§Ù‚'})
            fig.add_hline(y=0.8, line_dash="dash", line_color="green", annotation_text="Ø­Ø¯ Ø§Ù„Ù‚Ø¨ÙˆÙ„ Ø§Ù„Ø¢Ù„ÙŠ")
            fig.add_hline(y=0.2, line_dash="dash", line_color="red", annotation_text="Ø­Ø¯ Ø§Ù„Ø±ÙØ¶ Ø§Ù„Ø¢Ù„ÙŠ")
            fig.update_layout(yaxis_range=[0,1])
            st.plotly_chart(fig, use_container_width=True)
            
            st.markdown(f"""
            <div class="what-if-card">
                <strong>ğŸ” ØªØ­Ù„ÙŠÙ„:</strong><br>
                - Ø¹Ù†Ø¯ Ø§Ù„Ø¯Ø®Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠ ({income} Ø¬Ù†ÙŠÙ‡)ØŒ ÙØ±ØµØªÙƒ: {probs[np.abs(income_range - income).argmin()]*100:.1f}%<br>
                - Ø§Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù„ØªØ­Ù‚ÙŠÙ‚ 80% ÙØ±ØµØ©: {income_range[np.where(probs >= 0.8)[0][0]] if any(p >= 0.8 for p in probs) else 'Ù„Ø§ ÙŠÙ…ÙƒÙ†'} Ø¬Ù†ÙŠÙ‡<br>
                - Ø§Ù„Ø¯Ø®Ù„ Ø§Ù„Ø°ÙŠ ÙŠØ®ÙØ¶ Ø§Ù„ÙØ±ØµØ© Ù„Ø£Ù‚Ù„ Ù…Ù† 20%: {income_range[np.where(probs <= 0.2)[0][-1]] if any(p <= 0.2 for p in probs) else 'Ù„Ø§ ÙŠÙ…ÙƒÙ†'} Ø¬Ù†ÙŠÙ‡
            </div>
            """, unsafe_allow_html=True)

    # ===== ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù… =====
    st.markdown("---")
    st.markdown("## ğŸ“ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆÙÙ„Ø³ÙØ© Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©")
    
    # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ù„Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
    remote_areas = ["Ø£Ø³ÙŠÙˆØ·", "Ø³ÙˆÙ‡Ø§Ø¬", "Ù‚Ù†Ø§", "Ø£Ø³ÙˆØ§Ù†"]
    avg_weight_remote = data[data["Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©"].isin(remote_areas)]["ÙˆØ²Ù†_Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©"].mean()
    avg_weight_widowed = data[data["Ø§Ù„Ø­Ø§Ù„Ø©_Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©"] == "Ø£Ø±Ù…Ù„"]["ÙˆØ²Ù†_Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©"].mean()
    avg_weight_severe_disability = data[data["Ø´Ø¯Ø©_Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø©"] >= 0.7]["ÙˆØ²Ù†_Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©"].mean()
    avg_weight_intersectional = data[(data["Ø§Ù„Ø­Ø§Ù„Ø©_Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©"] == "Ø£Ø±Ù…Ù„") & 
                                      (data["Ø¥Ø¹Ø§Ù‚Ø©"] == 1) & 
                                      (data["Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©"].isin(remote_areas))]["ÙˆØ²Ù†_Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©"].mean()
    
    col1, col2 = st.columns(2)
    with col1:
        st.markdown("""
        <div style="background:white; padding:2rem; border-radius:15px; box-shadow:0 5px 20px rgba(0,0,0,0.05);">
            <h4>âœ¨ Ù†Ù‚Ø§Ø· Ø§Ù„Ù‚ÙˆØ© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØµÙ…ÙŠÙ…:</h4>
            <ul>
                <li><strong>Ù†Ø¸Ø§Ù… Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„ØªØµØ§Ø¹Ø¯ÙŠ:</strong> ÙŠØ¨Ø¯Ø£ Ù…Ù† Ø§Ù„Ø¹Ø¬Ø² Ø§Ù„Ø¨Ø¯Ù†ÙŠ (Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø´Ø¯ÙŠØ¯Ø©)ØŒ Ø«Ù… Ø§Ù„Ù‡Ø´Ø§Ø´Ø© Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ© (Ø§Ù„Ø£Ø±Ø§Ù…Ù„)ØŒ Ø«Ù… Ø§Ù„Ù…Ø¸Ù„ÙˆÙ…ÙŠØ© Ø§Ù„Ø¬ØºØ±Ø§ÙÙŠØ© (Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ù†Ø§Ø¦ÙŠØ©).</li>
                <li><strong>Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„ØªÙ‚Ø§Ø·Ø¹ÙŠØ©:</strong> ØªØ±Ø§ÙƒÙ… Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ù„Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø±ÙƒØ¨Ø© (Ø£Ø±Ù…Ù„Ø© + Ø¥Ø¹Ø§Ù‚Ø© + Ù…Ù†Ø·Ù‚Ø© Ù†Ø§Ø¦ÙŠØ©) ÙŠØ¹Ø·ÙŠ ÙˆØ²Ù†Ø§Ù‹ Ù…Ø¶Ø§Ø¹ÙØ§Ù‹ ÙŠØ¹ÙƒØ³ ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø­ÙŠØ§Ø©.</li>
                <li><strong>Ù…Ù‚ÙŠØ§Ø³ MCAS:</strong> Ù…ÙƒØ§ÙØ­Ø© Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„ÙˆÙ‡Ù…ÙŠØ© ÙˆØ§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù‚Ù„ÙŠÙ„Ø©.</li>
                <li><strong>Ù†Ø¸Ø§Ù… Ù‡Ø¬ÙŠÙ†:</strong> Ø§Ù„Ø¬Ù…Ø¹ Ø¨ÙŠÙ† Ø§Ù„Ø³Ø±Ø¹Ø© (Ø­Ø§Ù„Ø§Øª ÙˆØ§Ø¶Ø­Ø©) ÙˆØ§Ù„Ø¯Ù‚Ø© Ø§Ù„Ø¨Ø´Ø±ÙŠØ© (Ø­Ø§Ù„Ø§Øª Ø­Ø¯ÙŠØ©).</li>
                <li><strong>ØªØ­Ù„ÙŠÙ„ Ù…Ø§Ø°Ø§ Ù„Ùˆ:</strong> Ø´ÙØ§ÙÙŠØ© ÙƒØ§Ù…Ù„Ø© ØªØ³Ù…Ø­ Ù„Ù„Ù…Ø³Ø¤ÙˆÙ„ Ø¨ØªØ¬Ø±Ø¨Ø© Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ù…Ø®ØªÙ„ÙØ©.</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown(f"""
        <div style="background:white; padding:2rem; border-radius:15px; box-shadow:0 5px 20px rgba(0,0,0,0.05);">
            <h4>âš–ï¸ ØªØ£Ø«ÙŠØ± Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„ØªØµØ§Ø¹Ø¯ÙŠØ©:</h4>
            <ul>
                <li><strong>Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø´Ø¯ÙŠØ¯Ø©:</strong> Ù…ØªÙˆØ³Ø· Ø§Ù„ÙˆØ²Ù† {avg_weight_severe_disability:.2f}x</li>
                <li><strong>Ø§Ù„Ø£Ø±Ø§Ù…Ù„:</strong> Ù…ØªÙˆØ³Ø· Ø§Ù„ÙˆØ²Ù† {avg_weight_widowed:.2f}x</li>
                <li><strong>Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ù†Ø§Ø¦ÙŠØ©:</strong> Ù…ØªÙˆØ³Ø· Ø§Ù„ÙˆØ²Ù† {avg_weight_remote:.2f}x</li>
                <li><strong>Ø§Ù„ØªÙ‚Ø§Ø·Ø¹ (Ø£Ø±Ù…Ù„Ø© + Ø¥Ø¹Ø§Ù‚Ø© + Ù†Ø§Ø¦ÙŠØ©):</strong> Ù…ØªÙˆØ³Ø· Ø§Ù„ÙˆØ²Ù† {avg_weight_intersectional:.2f}x</li>
            </ul>
            <p>Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¹Ø§Ø¯Ù„ ÙŠØ­Ù‚Ù‚ ØªÙˆØ§Ø²Ù†Ø§Ù‹ Ø¨ÙŠÙ† Ø§Ù„ÙƒÙØ§Ø¡Ø© ÙˆØ§Ù„Ø¹Ø¯Ø§Ù„Ø©ØŒ ÙˆÙŠÙ‚Ù„Ù„ Ø§Ù„ÙØ¬ÙˆØ§Øª Ø¨ÙŠÙ† Ø§Ù„ÙØ¦Ø§Øª.</p>
        </div>
        """, unsafe_allow_html=True)

    # ØªØ°ÙŠÙŠÙ„
    st.markdown("""
    <div class="footer">
        <p>âš–ï¸ Mizan AI - Ù†Ø¸Ø§Ù… Ø§Ù„Ø¹Ø¯Ø§Ù„Ø© Ø§Ù„Ø°ÙƒÙŠ | Ù…Ø³ØªÙ†Ø¯ Ø¥Ù„Ù‰ Ø£Ø¨Ø­Ø§Ø« Ø¯. Ù…Ø­Ù…Ø¯ Ø§Ù„Ù‡Ø§Ø¯Ø§Ø¯ (MCAS) | Â© 2026</p>
        <p>ÙÙ„Ø³ÙØ© Ø§Ù„ØªØµÙ…ÙŠÙ…: Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© â† Ø§Ù„Ø£Ø±Ø§Ù…Ù„ â† Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ù†Ø§Ø¦ÙŠØ© â† Ø£ÙˆØ²Ø§Ù† Ù…ØªØ±Ø§ÙƒÙ…Ø© Ù„Ù„ØªØ¯Ø§Ø®Ù„Ø§Øª</p>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
